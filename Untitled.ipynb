{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from itertools import product\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True\n",
    "SYSTEM_DICTIONARY = '/usr/share/dict/words'\n",
    "vowels = set('aeiouy')\n",
    "alphabet = set('abcdefghijklmnopqrstuvwxyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IO\n",
    "\n",
    "def log(*args):\n",
    "    if VERBOSE: print (''.join([str(x) for x in args]))\n",
    "\n",
    "def words(text):\n",
    "    \"\"\"filter body of text for words\"\"\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def train(text, model=None):\n",
    "    \"\"\"generate or update a word model (dictionary of word:frequency)\"\"\"\n",
    "    model = collections.defaultdict(lambda: 0) if model is None else model\n",
    "    for word in words(text):\n",
    "        model[word] += 1\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from_files(file_list, model=None):\n",
    "    for f in file_list:\n",
    "        model = train(file(f).read(), model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILITY FUNCTIONS\n",
    "\n",
    "def numberofdupes(string, idx):\n",
    "    \"\"\"return the number of times in a row the letter at index idx is duplicated\"\"\"\n",
    "    # \"abccdefgh\", 2  returns 1\n",
    "    initial_idx = idx\n",
    "    last = string[idx]\n",
    "    while idx+1 < len(string) and string[idx+1] == last:\n",
    "        idx += 1\n",
    "    return idx-initial_idx\n",
    "\n",
    "def hamming_distance(word1, word2):\n",
    "    if word1 == word2:\n",
    "        return 0\n",
    "    dist = sum(imap(str.__ne__, word1[:len(word2)], word2[:len(word1)]))\n",
    "    dist = max([word2, word1]) if not dist else dist+abs(len(word2)-len(word1))\n",
    "    return dist\n",
    "\n",
    "def frequency(word, word_model):\n",
    "    return word_model.get(word, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSSIBILITIES ANALYSIS\n",
    "\n",
    "def variants(word):\n",
    "    \"\"\"get all possible variants for a word\"\"\"\n",
    "    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in splits if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    inserts    = [a + c + b for a, b in splits for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def double_variants(word):\n",
    "    \"\"\"get variants for the variants for a word\"\"\"\n",
    "    return set(s for w in variants(word) for s in variants(w))\n",
    "\n",
    "def reductions(word):\n",
    "    \"\"\"return flat option list of all possible variations of the word by removing duplicate letters\"\"\"\n",
    "    word = list(word)\n",
    "    # ['h','i', 'i', 'i'] becomes ['h', ['i', 'ii', 'iii']]\n",
    "    for idx, l in enumerate(word):\n",
    "        n = numberofdupes(word, idx)\n",
    "        # if letter appears more than once in a row\n",
    "        if n:\n",
    "            # generate a flat list of options ('hhh' becomes ['h','hh','hhh'])\n",
    "            flat_dupes = [l*(r+1) for r in xrange(n+1)][:3] # only take up to 3, there are no 4 letter repetitions in english\n",
    "            # remove duplicate letters in original word\n",
    "            for _ in range(n):\n",
    "                word.pop(idx+1)\n",
    "            # replace original letter with flat list\n",
    "            word[idx] = flat_dupes\n",
    "\n",
    "    # ['h',['i','ii','iii']] becomes 'hi','hii','hiii'\n",
    "    for p in product(*word):\n",
    "        yield ''.join(p)\n",
    "\n",
    "def vowelswaps(word):\n",
    "    \"\"\"return flat option list of all possible variations of the word by swapping vowels\"\"\"\n",
    "    word = list(word)\n",
    "    # ['h','i'] becomes ['h', ['a', 'e', 'i', 'o', 'u', 'y']]\n",
    "    for idx, l in enumerate(word):\n",
    "        if type(l) == list:\n",
    "            pass                        # dont mess with the reductions\n",
    "        elif l in vowels:\n",
    "            word[idx] = list(vowels)    # if l is a vowel, replace with all possible vowels\n",
    "\n",
    "    # ['h',['i','ii','iii']] becomes 'hi','hii','hiii'\n",
    "    for p in product(*word):\n",
    "        yield ''.join(p)\n",
    "\n",
    "def both(word):\n",
    "    \"\"\"permute all combinations of reductions and vowelswaps\"\"\"\n",
    "    for reduction in reductions(word):\n",
    "        for variant in vowelswaps(reduction):\n",
    "            yield variant\n",
    "\n",
    "### POSSIBILITY CHOOSING\n",
    "\n",
    "def suggestions(word, real_words, short_circuit=True):\n",
    "    \"\"\"get best spelling suggestion for word\n",
    "    return on first match if short_circuit is true, otherwise collect all possible suggestions\"\"\"\n",
    "    word = word.lower()\n",
    "    if short_circuit:   # setting short_circuit makes the spellchecker much faster, but less accurate in some cases\n",
    "        return ({word}                      & real_words or   #  caps     \"inSIDE\" => \"inside\"\n",
    "                set(reductions(word))       & real_words or   #  repeats  \"jjoobbb\" => \"job\"\n",
    "                set(vowelswaps(word))       & real_words or   #  vowels   \"weke\" => \"wake\"\n",
    "                set(variants(word))         & real_words or   #  other    \"nonster\" => \"monster\"\n",
    "                set(both(word))             & real_words or   #  both     \"CUNsperrICY\" => \"conspiracy\"\n",
    "                set(double_variants(word))  & real_words or   #  other    \"nmnster\" => \"manster\"\n",
    "                {\"NO SUGGESTION\"})\n",
    "    else:\n",
    "        return ({word}                      & real_words or\n",
    "                (set(reductions(word))  | set(vowelswaps(word)) | set(variants(word)) | set(both(word)) | set(double_variants(word))) & real_words or\n",
    "                {\"NO SUGGESTION\"})\n",
    "\n",
    "def best(inputted_word, suggestions, word_model=None):\n",
    "    \"\"\"choose the best suggestion in a list based on lowest hamming distance from original word, or based on frequency if word_model is provided\"\"\"\n",
    "\n",
    "    suggestions = list(suggestions)\n",
    "\n",
    "    def comparehamm(one, two):\n",
    "        score1 = hamming_distance(inputted_word, one)\n",
    "        score2 = hamming_distance(inputted_word, two)\n",
    "        return cmp(score1, score2)  # lower is better\n",
    "\n",
    "    def comparefreq(one, two):\n",
    "        score1 = frequency(one, word_model)\n",
    "        score2 = frequency(two, word_model)\n",
    "        return cmp(score2, score1)  # higher is better\n",
    "\n",
    "    freq_sorted = sorted(suggestions, cmp=comparefreq)[10:]     # take the top 10\n",
    "    hamming_sorted = sorted(suggestions, cmp=comparehamm)[10:]  # take the top 10\n",
    "    print ('FREQ', freq_sorted)\n",
    "    print ('HAM', hamming_sorted)\n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9af699c38af4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# init the word frequency model with a simple list of all possible words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mword_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSYSTEM_DICTIONARY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mreal_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # init the word frequency model with a simple list of all possible words\n",
    "    word_model = train(open(json.loads(SYSTEM_DICTIONARY)))\n",
    "    real_words = set(word_model)\n",
    "\n",
    "    # add other texts here, they are used to train the word frequency model\n",
    "    texts = [\n",
    "        'sherlockholmes.txt',\n",
    "        'lemmas.txt',\n",
    "    ]\n",
    "    # enhance the model with real bodies of english so we know which words are more common than others\n",
    "    word_model = train_from_files(texts, word_model)\n",
    "\n",
    "    log('Total Word Set: ', len(word_model))\n",
    "    log('Model Precision: %s' % (float(sum(word_model.values()))/len(word_model)))\n",
    "    try:\n",
    "        while True:\n",
    "            word = str(raw_input('>'))\n",
    "\n",
    "            possibilities = suggestions(word, real_words, short_circuit=False)\n",
    "            short_circuit_result = suggestions(word, real_words, short_circuit=True)\n",
    "            if VERBOSE:\n",
    "                print ([(x, word_model[x]) for x in possibilities])\n",
    "                print (best(word, possibilities, word_model))\n",
    "                print ('---')\n",
    "            print ([(x, word_model[x]) for x in short_circuit_result])\n",
    "            if VERBOSE:\n",
    "                print (best(word, short_circuit_result, word_model))\n",
    "\n",
    "    except (EOFError, KeyboardInterrupt):\n",
    "        exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
